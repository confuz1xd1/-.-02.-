## 1. Полное задание (вариант 7)

Создать рекуррентную нейронную сеть (RNN) для генерации текста на основе обучающего набора, реализовав её в Python с использованием фреймворка TensorFlow/Keras. [web:23][web:25]  
Нужно построить модель символ‑уровневой генерации текста с Embedding‑слоем для векторизации символов, одним или несколькими LSTM‑слоями размерности 128 и выходным полносвязным слоем с softmax для предсказания вероятности следующего символа. [web:17][web:34]  
Требуется реализовать предобработку текста (словарь символов, кодирование текста в последовательности фиксированной длины, формирование пар «входная последовательность – следующий символ»), обучить модель на выбранной книге в формате txt и реализовать генерацию текста с управлением температурой, демонстрируя влияние параметра *temperature* на разнообразие результата. [web:17][web:25]

---

## 2. Алгоритм работы нейросети по блокам

1. **Подготовка данных**  
   * Текст книги считывается как одна строка, из неё формируется множество уникальных символов и создаются отображения «символ → индекс» и «индекс → символ», после чего строка переводится в массив индексов. [web:17][web:25]  
   * По этому массиву скользящим окном длины `sequence_length` формируются обучающие пары: входные последовательности индексов и целевые символы, идущие сразу после каждой последовательности. [web:17][web:25]

2. **Обучение модели**  
   * Каждая входная последовательность проходит через Embedding‑слой, заменяющий индексы символов их плотными векторами фиксированной размерности. [web:17][web:31]  
   * Векторы по шагам обрабатываются LSTM‑слоями, которые обновляют скрытое состояние, после чего финальное состояние поступает в Dense‑слой с softmax; по этому распределению вероятностей считается ошибка `sparse_categorical_crossentropy`, и веса обновляются оптимизатором Adam. [web:17][web:31][web:40]

3. **Генерация текста**  
   * Начальный `seed`‑текст кодируется в индексы и подаётся в модель для получения распределения вероятностей следующего символа. [web:17][web:28]  
   * Логиты делятся на температуру `T` (*temperature‑based sampling*), по скорректированному распределению случайно выбирается следующий символ, добавляется к последовательности, и цикл «предсказание → выбор → добавление» повторяется заданное число раз. [web:17][web:28]

---

## 3. Ответ на контрольный вопрос №7

Оптимальный размер блока для алгоритмов во внешней памяти выбирают так, чтобы время последовательного чтения данных из блока существенно превышало время позиционирования головки к этому блоку, то есть накладные расходы на перемещение амортизируются большим объёмом полезных данных. [web:29]  
На практике размер блока делают значительно больше минимального аппаратного (кратным нескольким или десяткам килобайт), чтобы уменьшить число обращений к диску и повысить пропускную способность, но не настолько большим, чтобы чрезмерно возрастали объём лишне прочитанных данных и требования к кэш‑памяти. [web:29][web:41]
